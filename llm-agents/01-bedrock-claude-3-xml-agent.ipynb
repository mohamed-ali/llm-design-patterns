{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9e9e65-abda-41c6-a7e5-5e0c985443ca",
   "metadata": {},
   "source": [
    "# Build an LLM agent with Claude 3 and Langchain default XML agent configuration\n",
    "\n",
    "LLM Agents are one of the most impactful design patterns that emerged recently, because it provides a general purpose framework for complex problem solving and task automation. In this notebook, we'll explore how to build an LLM agent that leverages the capabilities of Claude 3 and the default XML agent configuration provided by the LangChain library.\n",
    "\n",
    "We will also explore the limitations of the default langchain xml agent, and reflect how to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a4d190-4608-4b23-8406-fa50d7cee7be",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0ec076-540f-4f4a-8f9b-7df59fb0725e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install duckduckgo-search -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2012055-101d-4d25-bea3-3cf914a91676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdebc7e1-3f13-48ea-9379-f2166663ccfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U langchainhub langchain-community langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c312573-5281-49ac-84d6-f03e1c66ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-west-2\",\n",
    ")\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=\"us-west-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afcf8654-b7bf-4bb8-9e00-dd8f459ecb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_models = bedrock.list_foundation_models()[\"modelSummaries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82680d9e-b08f-483b-a5eb-7f2dffeee028",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_models = [model for model in foundation_models if \"Claude\" in model[\"modelName\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7042707-f2da-4473-9ef2-192f2c6bd937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-opus-20240229-v1:0',\n",
       " 'modelId': 'anthropic.claude-3-opus-20240229-v1:0',\n",
       " 'modelName': 'Claude 3 Opus',\n",
       " 'providerName': 'Anthropic',\n",
       " 'inputModalities': ['TEXT', 'IMAGE'],\n",
       " 'outputModalities': ['TEXT'],\n",
       " 'responseStreamingSupported': True,\n",
       " 'customizationsSupported': [],\n",
       " 'inferenceTypesSupported': ['ON_DEMAND'],\n",
       " 'modelLifecycle': {'status': 'ACTIVE'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claude_models[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158827fb-cb58-45b5-9394-d1177df13a11",
   "metadata": {},
   "source": [
    "Here's the list of available Claude model ids, we can use the Claude 3 family of models for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa6d939-0673-4cab-91f3-6b47ef768e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anthropic.claude-instant-v1:2:100k',\n",
       " 'anthropic.claude-instant-v1',\n",
       " 'anthropic.claude-v2:0:18k',\n",
       " 'anthropic.claude-v2:0:100k',\n",
       " 'anthropic.claude-v2:1:18k',\n",
       " 'anthropic.claude-v2:1:200k',\n",
       " 'anthropic.claude-v2:1',\n",
       " 'anthropic.claude-v2',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       " 'anthropic.claude-3-opus-20240229-v1:0']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model[\"modelId\"] for model in claude_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cf913-771a-4c85-9083-65aa92efb21d",
   "metadata": {},
   "source": [
    "# Initialize Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b87751c-fd8c-4c38-9163-ca52d3fdd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_xml_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33d8115a-2eff-4187-b6ce-4248f3172555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DuckDuckGoSearchRun()]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b1a92-27d0-4d39-a555-cf2f012e4e9c",
   "metadata": {},
   "source": [
    "The tool object will be rendered into a text representation that will be injected in the agent prompt.\n",
    "Part of the tool definition is a natural language description that outlines what the tool is, what it can be used for, and what it's input should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c723c6dc-e69c-4c05-a5ad-58f526208d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.render import ToolsRenderer, render_text_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23873edf-4a72-43af-a551-6873e7d24e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckduckgo_search - A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_text_description(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b71ccb-c560-44ce-8e46-bcb24ed89a1f",
   "metadata": {},
   "source": [
    "## Define the Agent Prompt\n",
    "\n",
    "An LLM agent is not formed by making a single call to an LLM, it is rather a system made up of several components that co-operate to give us the overall impression of an intelligent agent. These components are:\n",
    "\n",
    "1. Agent Planner: Uses the prompt to ask the LLM to plan the next action/tool call that needs to happen to answer the user question and perform the required automation.\n",
    "2. Agent Executor: Responsible for coordinating the execution of the planned actions, invoking the necessary tools, and handling the overall flow of the agent's decision-making process.\n",
    "3. Tools: The external capabilities that the agent can leverage to gather information, perform calculations, or take other actions to accomplish the user's request.\n",
    "4. Output Parser: Responsible for interpreting the LLM responses to identify, extract the tools input if we a tool invocation is required, or the final answer.\n",
    "5. LLM: The large language model, in this case, the Claude 3 model from Anthropic, which serves as the core intelligence of the agent and provides the reasoning and language generation capabilities.\n",
    "\n",
    "The aim of the agent desgin pattern is to prompt an LLM to think step by step about a question or task, while interleaving the ability to leverage external tools to perform partial steps or retrieve information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dafcd26-88c1-4f6f-b80f-1f20e9c577a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0cb6d9-f9ca-4d61-8839-2d46c76e9488",
   "metadata": {},
   "source": [
    "Below we explore few variants of the agent LLM prompt, gradually being refactoring to enhance how the agent uses the conversation history and to reduce the number of tokens\n",
    "in the prompt to reduce the cost and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e00ac05-21e7-4dca-ad38-f2abc81160ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_v1 = \"\"\"You are a helpful assistant. Help the user answer any questions.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "To answer a user question you can use a tool, or leverage the information inside the conversation history within <history> tag.\n",
    "In order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\n",
    "For example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:\n",
    "\n",
    "<tool>search</tool><tool_input>weather in SF</tool_input>\n",
    "<observation>64 degrees</observation>\n",
    "\n",
    "When you are done, respond with a final answer in markdown between <final_answer></final_answer>.\n",
    "For example: <final_answer>The weather in SF is 64 degrees</final_answer>\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous Conversation:\n",
    "<history>\n",
    "{chat_history}\n",
    "</history>\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6621d58-c448-4276-89a9-3343fd70c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_v2 = \"\"\"\n",
    "You are a helpful assistant. Help the user answer any questions, leveraging the information from the conversation history within the <history> tag whenever possible to avoid duplicating work.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "To answer a user question, you can either:\n",
    "\n",
    "1. Reuse relevant information from the conversation history, or\n",
    "2. Use one or multiple tools\n",
    "\n",
    "To use a tool, follow this format:\n",
    "\n",
    "<thinking>Think about what tool to use</thinking>\n",
    "<tool>tool name</tool><tool_input>valid input to the tool</tool_input>.\n",
    "<observation>You will then get back a response inside these observation tags</observation>\n",
    "\n",
    "When you are done, respond with a final answer between <final_answer></final_answer>.\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous Conversation:\n",
    "<history>\n",
    "{chat_history}\n",
    "</history>\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d011b73d-7f2d-4cde-aef0-aababcc78042",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_v3 = \"\"\"\n",
    "You are a helpful assistant. Leverage the <history> to avoid duplicating work when answering questions.\n",
    "\n",
    "Available tools:\n",
    "<tools>\n",
    "{tools}\n",
    "</tools>\n",
    "\n",
    "To answer, first review the <history>. If insufficient use tool(s) with the following format:\n",
    "<thinking>Think about which tool(s) to use and why</thinking>\n",
    "<tool>tool_name</tool><tool_input>input</tool_input><observation>response</observation>\n",
    "\n",
    "When you are done, provide a final answer in markdown within <final_answer></final_answer>.\n",
    "\n",
    "<history>\n",
    "{chat_history}\n",
    "</history>\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a96e3449-9d21-4170-94d9-3adec19eac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"agent_scratchpad\", \"chat_history\", \"input\", \"tools\"],\n",
    "    template=prompt_template_v3\n",
    ")\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=prompt_template\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"agent_scratchpad\", \"input\", \"tools\"],\n",
    "    partial_variables={\"chat_history\": \"\"},\n",
    "    messages=[human_message_prompt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e83ebd7-8ca6-4dc0-9cee-d05848eb60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "model_kwargs =  { \n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "\n",
    "model = ChatBedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "417ac27e-8f93-43f3-94d8-968c5efa39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the XML Agent\n",
    "agent = create_xml_agent(\n",
    "    model, tools, prompt,\n",
    "    stop_sequence=[\"</tool_input>\", \"</final_answer>\"]\n",
    ")\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, return_intermediate_steps=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98e93708-eedb-42d2-be17-5db6cab7823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<thinking>To answer this question, I will first search for information about \"Amazon Bedrock\" on DuckDuckGo to see if I can find relevant details.</thinking>\n",
      "\n",
      "<tool>duckduckgo_search</tool><tool_input>Amazon Bedrock\u001b[0m\u001b[36;1m\u001b[1;3mWith Amazon Bedrock Custom Model Import, organizations can now import and access their own custom models as a fully managed Application Programming Interface (API) in Bedrock, giving them unprecedented choice when building generative AI applications.To get started, organizations can easily add models to Amazon Bedrock that they customized on Amazon SageMaker, or via another third-party tool or ... Update October 10, 2023 — Amazon Bedrock is now available in 3 regions globally: US East (N. Virginia), US West (Oregon), and Asia Pacific (Tokyo). This April, we announced Amazon Bedrock as part of a set of new tools for building with generative AI on AWS.Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies ... Amazon Bedrock, the easiest way to build and scale generative AI applications with foundation models (FMs), is now generally available. Amazon Bedrock is a fully managed service that offers a choice of high-performing FMs from leading AI companies including AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon, along with a broad set of capabilities that you need to build generative AI ... Amazon Bedrock is the first fully managed generative AI service to offer Llama 2, Meta's next-generation LLM, through a managed API. Llama 2 models come with significant improvements over the original Llama models, including being trained on 40% more data and having a longer context length of 4,000 tokens to work with larger documents ... We introduced Amazon Bedrock to the world a little over a year ago, delivering an entirely new way to build generative artificial intelligence (AI) applications. With the broadest selection of first- and third-party foundation models (FMs) as well as user-friendly capabilities, Amazon Bedrock is the fastest and easiest way to build and scale secure generative […]\u001b[0m\u001b[32;1m\u001b[1;3m<thinking>The search results provide a good overview of what Amazon Bedrock is, including that it is a fully managed service that offers a choice of high-performing foundation models from leading AI companies. It also mentions that Bedrock now supports the Llama 2 model from Meta. I believe I have enough information to provide a final answer to the question.</thinking>\n",
      "\n",
      "<final_answer>\n",
      "Amazon Bedrock is a fully managed service from Amazon Web Services (AWS) that allows organizations to build and scale generative AI applications. Key features of Amazon Bedrock include:\n",
      "\n",
      "- Offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon.\n",
      "- Provides a managed API to access and use these foundation models, making it easier to build generative AI applications.\n",
      "- Supports the latest Llama 2 model from Meta, which has significant improvements over the original Llama models.\n",
      "- Includes a broad set of capabilities needed to build and scale generative AI applications, such as model management, scaling, and security.\n",
      "\n",
      "In summary, Amazon Bedrock is a fully managed service that gives organizations the ability to easily build and scale generative AI applications by providing access to a variety of high-quality foundation models through a managed API.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: user 248 ms, sys: 19.3 ms, total: 267 ms\n",
      "Wall time: 5.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Invoke XML Agent\n",
    "response = agent_executor.invoke({\"input\": \"What is Amazon Bedrock?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8683dce2-1354-46df-a6fd-ae02884571ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amazon Bedrock is a fully managed service from Amazon Web Services (AWS) that allows organizations to build and scale generative AI applications. Key features of Amazon Bedrock include:\n",
      "\n",
      "- Offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon.\n",
      "- Provides a managed API to access and use these foundation models, making it easier to build generative AI applications.\n",
      "- Supports the latest Llama 2 model from Meta, which has significant improvements over the original Llama models.\n",
      "- Includes a broad set of capabilities needed to build and scale generative AI applications, such as model management, scaling, and security.\n",
      "\n",
      "In summary, Amazon Bedrock is a fully managed service that gives organizations the ability to easily build and scale generative AI applications by providing access to a variety of high-quality foundation models through a managed API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b8a8059-ccce-4aa3-9982-7269310222c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool='duckduckgo_search' tool_input='Amazon Bedrock' log='<thinking>To answer this question, I will first search for information about \"Amazon Bedrock\" on DuckDuckGo to see if I can find relevant details.</thinking>\\n\\n<tool>duckduckgo_search</tool><tool_input>Amazon Bedrock'\n",
      "With Amazon Bedrock Custom Model Import, organizations can now import and access their own custom models as a fully managed Application Programming Interface (API) in Bedrock, giving them unprecedented choice when building generative AI applications.To get started, organizations can easily add models to Amazon Bedrock that they customized on Amazon SageMaker, or via another third-party tool or ... Update October 10, 2023 — Amazon Bedrock is now available in 3 regions globally: US East (N. Virginia), US West (Oregon), and Asia Pacific (Tokyo). This April, we announced Amazon Bedrock as part of a set of new tools for building with generative AI on AWS.Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies ... Amazon Bedrock, the easiest way to build and scale generative AI applications with foundation models (FMs), is now generally available. Amazon Bedrock is a fully managed service that offers a choice of high-performing FMs from leading AI companies including AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon, along with a broad set of capabilities that you need to build generative AI ... Amazon Bedrock is the first fully managed generative AI service to offer Llama 2, Meta's next-generation LLM, through a managed API. Llama 2 models come with significant improvements over the original Llama models, including being trained on 40% more data and having a longer context length of 4,000 tokens to work with larger documents ... We introduced Amazon Bedrock to the world a little over a year ago, delivering an entirely new way to build generative artificial intelligence (AI) applications. With the broadest selection of first- and third-party foundation models (FMs) as well as user-friendly capabilities, Amazon Bedrock is the fastest and easiest way to build and scale secure generative […]\n"
     ]
    }
   ],
   "source": [
    "print(response['intermediate_steps'][0][0])\n",
    "print(response['intermediate_steps'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9dabf9f-9905-4d94-8e87-9652b01c5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<thinking>The history provided gives a good overview of what Amazon Bedrock is, so I don't need to use any additional tools to answer this question. The key points are:\n",
      "\n",
      "- Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies.\n",
      "- It allows organizations to import and access their own custom models as a fully managed API, giving them more choice when building generative AI applications.\n",
      "- Amazon Bedrock is now generally available in 3 regions globally and includes support for the latest Llama 2 models from Meta.\n",
      "- It provides an easy way to build and scale secure generative AI applications using a variety of foundation models.\n",
      "</thinking>\n",
      "\n",
      "<final_answer>\n",
      "Amazon Bedrock is a fully managed service from AWS that allows organizations to build and scale generative AI applications. It offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon.\n",
      "\n",
      "Key features of Amazon Bedrock include:\n",
      "\n",
      "- Ability to import and access custom models as a fully managed API, giving organizations more choice when building generative AI apps.\n",
      "- Support for the latest Llama 2 models from Meta, which have significant improvements over the original Llama models.\n",
      "- Broad set of capabilities to make it easy to build and scale secure generative AI applications.\n",
      "- General availability in 3 regions globally: US East (N. Virginia), US West (Oregon), and Asia Pacific (Tokyo).\n",
      "\n",
      "Overall, Amazon Bedrock provides an easy and flexible way for organizations to leverage the power of generative AI in their applications.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: user 183 ms, sys: 26.9 ms, total: 210 ms\n",
      "Wall time: 3.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Using with chat history\n",
    "# from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "response_with_history = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is Amazon Bedrock?\",\n",
    "        # Notice that chat_history is a string\n",
    "        # since this prompt is aimed at LLMs, not chat models\n",
    "        \"chat_history\": f\"Human: What is Amazon Bedrock?\\nAI: {response['intermediate_steps'][0][1]}\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0728ed75-6015-4b25-bb10-a07fcbc32587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amazon Bedrock is a fully managed service from AWS that allows organizations to build and scale generative AI applications. It offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon.\n",
      "\n",
      "Key features of Amazon Bedrock include:\n",
      "\n",
      "- Ability to import and access custom models as a fully managed API, giving organizations more choice when building generative AI apps.\n",
      "- Support for the latest Llama 2 models from Meta, which have significant improvements over the original Llama models.\n",
      "- Broad set of capabilities to make it easy to build and scale secure generative AI applications.\n",
      "- General availability in 3 regions globally: US East (N. Virginia), US West (Oregon), and Asia Pacific (Tokyo).\n",
      "\n",
      "Overall, Amazon Bedrock provides an easy and flexible way for organizations to leverage the power of generative AI in their applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response_with_history[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ccc690e-7855-432c-9e29-c0483007ebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<thinking>\n",
      "The history provided does not mention anything about an \"Amazon Bedrock Knowledge Base\". The information given focuses on Amazon Bedrock being a fully managed service that provides access to various foundation models from different AI companies, as well as capabilities for building and scaling generative AI applications. To answer this question, I will need to perform a search to find more information about an \"Amazon Bedrock Knowledge Base\".\n",
      "</thinking>\n",
      "\n",
      "<tool>duckduckgo_search</tool><tool_input>amazon bedrock knowledge base\u001b[0m\u001b[36;1m\u001b[1;3mLearn how to use Knowledge Bases for Amazon Bedrock, a fully managed capability that connects foundation models to internal data sources for Retrieval Augmented Generation (RAG). RAG enhances FM responses with contextual and relevant company data from Amazon S3 or other vector databases. For instructions to create one, see Create a knowledge base. Configure the maximum number of results using the console. To use the maximum number of results option using the console, complete the following steps: On the Amazon Bedrock console, choose Knowledge bases in the left navigation pane. Select the knowledge base you created. This feature makes evaluation of multiple vector stores seamless. You would start by providing the name for KB in the step 1. KB-Step-1. In the Step 2, you would create a data source, which currently allows S3 to be chosen as a location from where you can ingest data as shown below. KB-Step-2. In Step 3, you would select the Vector Data Store. Create an Agent. On the Bedrock console, under Orchestration, click Agents and then Create Agent. 2. Provide Agent details. You can either create a new service role or use an existing service role ... With a knowledge base, you can securely connect foundation models (FMs) in Amazon Bedrock to your company data for Retrieval Augmented Generation (RAG). Access to additional data helps the model generate more relevant, context-specific, and accurate responses without continuously retraining the FM. Knowledge Bases gives you a fully managed RAG ...\u001b[0m\u001b[32;1m\u001b[1;3m<thinking>\n",
      "The original question is about what the Amazon Bedrock Knowledge Base is. The history provided some information about Amazon Bedrock in general, but did not specifically cover the Knowledge Base feature. To answer this question, I will need to search for more information on the Amazon Bedrock Knowledge Base specifically.\n",
      "</thinking>\n",
      "\n",
      "<tool>duckduckgo_search</tool><tool_input>amazon bedrock knowledge base\u001b[0m\u001b[36;1m\u001b[1;3mLearn how to use Knowledge Bases for Amazon Bedrock, a fully managed capability that connects foundation models to internal data sources for Retrieval Augmented Generation (RAG). RAG enhances FM responses with contextual and relevant company data from Amazon S3 or other vector databases. This feature makes evaluation of multiple vector stores seamless. You would start by providing the name for KB in the step 1. KB-Step-1. In the Step 2, you would create a data source, which currently allows S3 to be chosen as a location from where you can ingest data as shown below. KB-Step-2. In Step 3, you would select the Vector Data Store. Create an Agent. On the Bedrock console, under Orchestration, click Agents and then Create Agent. 2. Provide Agent details. You can either create a new service role or use an existing service role ... With a knowledge base, you can securely connect foundation models (FMs) in Amazon Bedrock to your company data for Retrieval Augmented Generation (RAG). Access to additional data helps the model generate more relevant, context-specific, and accurate responses without continuously retraining the FM. Knowledge Bases gives you a fully managed RAG ... Knowledge Bases for Amazon Bedrock makes it even easier to build generative AI applications that use proprietary data to deliver customized, up-to-date responses. Customers have more options to customize models in Amazon Bedrock with fine-tuning support for Cohere Command, Meta Llama 2, and Amazon Titan models, with Anthropic's Claude coming soon\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>\n",
      "Amazon Bedrock Knowledge Base is a fully managed capability within Amazon Bedrock that allows you to connect foundation models (FMs) to your internal data sources for Retrieval Augmented Generation (RAG). RAG enhances the responses from the FMs with contextual and relevant data from your company's data sources, such as Amazon S3 or other vector databases.\n",
      "\n",
      "The key features of Amazon Bedrock Knowledge Base include:\n",
      "\n",
      "- Seamless integration of multiple vector stores for data sources\n",
      "- Ability to ingest data from Amazon S3 and other vector databases\n",
      "- Fully managed RAG capabilities to enhance FM responses with your company data\n",
      "- Customization options to fine-tune FMs like Cohere Command, Meta Llama 2, and Amazon Titan models (with Anthropic's Claude coming soon)\n",
      "\n",
      "With Amazon Bedrock Knowledge Base, you can build generative AI applications that leverage your proprietary data to deliver more relevant, context-specific, and accurate responses, without the need to continuously retrain the foundation models.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: user 288 ms, sys: 25.3 ms, total: 313 ms\n",
      "Wall time: 7.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Using with chat history\n",
    "# from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "response_with_history = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"what's Amazon Bedrock Knowledge Base?\",\n",
    "        # Notice that chat_history is a string\n",
    "        # since this prompt is aimed at LLMs, not chat models\n",
    "        \"chat_history\": f\"Human: What is Amazon Bedrock?\\nAI: {response['intermediate_steps'][0][1]}\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e914c09-efb6-4d44-9805-e0f13c3f4a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amazon Bedrock Knowledge Base is a fully managed capability within Amazon Bedrock that allows you to connect foundation models (FMs) to your internal data sources for Retrieval Augmented Generation (RAG). RAG enhances the responses from the FMs with contextual and relevant data from your company's data sources, such as Amazon S3 or other vector databases.\n",
      "\n",
      "The key features of Amazon Bedrock Knowledge Base include:\n",
      "\n",
      "- Seamless integration of multiple vector stores for data sources\n",
      "- Ability to ingest data from Amazon S3 and other vector databases\n",
      "- Fully managed RAG capabilities to enhance FM responses with your company data\n",
      "- Customization options to fine-tune FMs like Cohere Command, Meta Llama 2, and Amazon Titan models (with Anthropic's Claude coming soon)\n",
      "\n",
      "With Amazon Bedrock Knowledge Base, you can build generative AI applications that leverage your proprietary data to deliver more relevant, context-specific, and accurate responses, without the need to continuously retrain the foundation models.\n",
      "\n",
      "tool='duckduckgo_search' tool_input='amazon bedrock knowledge base' log='<thinking>\\nThe history provided does not mention anything about an \"Amazon Bedrock Knowledge Base\". The information given focuses on Amazon Bedrock being a fully managed service that provides access to various foundation models from different AI companies, as well as capabilities for building and scaling generative AI applications. To answer this question, I will need to perform a search to find more information about an \"Amazon Bedrock Knowledge Base\".\\n</thinking>\\n\\n<tool>duckduckgo_search</tool><tool_input>amazon bedrock knowledge base'\n",
      "Learn how to use Knowledge Bases for Amazon Bedrock, a fully managed capability that connects foundation models to internal data sources for Retrieval Augmented Generation (RAG). RAG enhances FM responses with contextual and relevant company data from Amazon S3 or other vector databases. For instructions to create one, see Create a knowledge base. Configure the maximum number of results using the console. To use the maximum number of results option using the console, complete the following steps: On the Amazon Bedrock console, choose Knowledge bases in the left navigation pane. Select the knowledge base you created. This feature makes evaluation of multiple vector stores seamless. You would start by providing the name for KB in the step 1. KB-Step-1. In the Step 2, you would create a data source, which currently allows S3 to be chosen as a location from where you can ingest data as shown below. KB-Step-2. In Step 3, you would select the Vector Data Store. Create an Agent. On the Bedrock console, under Orchestration, click Agents and then Create Agent. 2. Provide Agent details. You can either create a new service role or use an existing service role ... With a knowledge base, you can securely connect foundation models (FMs) in Amazon Bedrock to your company data for Retrieval Augmented Generation (RAG). Access to additional data helps the model generate more relevant, context-specific, and accurate responses without continuously retraining the FM. Knowledge Bases gives you a fully managed RAG ...\n"
     ]
    }
   ],
   "source": [
    "print(response_with_history['output'])\n",
    "print(response_with_history['intermediate_steps'][0][0])\n",
    "print(response_with_history['intermediate_steps'][0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
